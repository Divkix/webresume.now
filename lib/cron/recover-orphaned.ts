/**
 * Shared orphaned resume recovery logic.
 *
 * Called by:
 * - worker.ts scheduled handler (direct invocation, no extra Worker billing)
 * - /api/cron/recover-orphaned route handler (manual trigger via HTTP)
 *
 * Finds resumes stuck in pending_claim status that have valid r2Key and fileHash
 * but weren't successfully queued (e.g., due to worker crash after upload).
 */

import { and, eq, inArray, isNotNull, lt, sql } from "drizzle-orm";
import type { Database } from "@/lib/db";
import { resumes } from "@/lib/db/schema";
import { publishResumeParse } from "@/lib/queue/resume-parse";
import type { ResumeParseMessage } from "@/lib/queue/types";

export interface RecoverOrphanedResult {
  ok: true;
  recovered: number;
  found: number;
  timestamp: string;
}

export async function recoverOrphanedResumes(
  db: Database,
  queue: Queue<ResumeParseMessage>,
): Promise<RecoverOrphanedResult> {
  // Find orphaned resumes older than 5 minutes
  // (gives normal flow time to complete)
  const fiveMinutesAgo = new Date(Date.now() - 5 * 60 * 1000).toISOString();

  const orphanedResumes = await db
    .select({
      id: resumes.id,
      userId: resumes.userId,
      r2Key: resumes.r2Key,
      fileHash: resumes.fileHash,
      totalAttempts: resumes.totalAttempts,
    })
    .from(resumes)
    .where(
      and(
        eq(resumes.status, "pending_claim"),
        isNotNull(resumes.r2Key),
        isNotNull(resumes.fileHash),
        lt(resumes.createdAt, fiveMinutesAgo),
      ),
    )
    .limit(10); // Process max 10 per run to avoid overwhelming queue

  if (orphanedResumes.length === 0) {
    return {
      ok: true,
      recovered: 0,
      found: 0,
      timestamp: new Date().toISOString(),
    };
  }

  const now = new Date().toISOString();
  const successfulIds: string[] = [];

  // Process queue publishes and collect successful IDs
  for (const resume of orphanedResumes) {
    // Skip if already at max attempts (6 total = 3 queue retries x 2 manual retries)
    if ((resume.totalAttempts ?? 0) >= 6) {
      console.log(`Skipping resume ${resume.id} - max attempts reached`);
      continue;
    }

    try {
      // Re-publish to queue
      await publishResumeParse(queue, {
        resumeId: resume.id,
        userId: resume.userId,
        r2Key: resume.r2Key,
        fileHash: resume.fileHash as string,
        attempt: (resume.totalAttempts ?? 0) + 1,
      });

      successfulIds.push(resume.id);
      console.log(`Recovered orphaned resume: ${resume.id}`);
    } catch (error) {
      console.error(`Failed to recover resume ${resume.id}:`, error);
    }
  }

  // Batch update all successful resumes in single DB call
  if (successfulIds.length > 0) {
    await db
      .update(resumes)
      .set({
        status: "queued",
        queuedAt: now,
        totalAttempts: sql`${resumes.totalAttempts} + 1`,
      })
      .where(inArray(resumes.id, successfulIds));
  }

  const recovered = successfulIds.length;

  return {
    ok: true,
    recovered,
    found: orphanedResumes.length,
    timestamp: now,
  };
}
